{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDAV+SHAP+1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKJWQH+H1CAUFBGQniCzEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bozorgpanah/The-Explainable-Machine-Learning-Model-withPrivacy/blob/main/Paper1/MDAV%2BSHAP%2B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdzxKQoKaVVe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import scipy\n",
        "%matplotlib inline\n",
        "\n",
        "#Define a Artificial Data set comprise 1 million records, 10 variables, 5 informative variables \n",
        "X, y = make_classification(n_samples=100, n_features=10, n_redundant=3, n_repeated=2, \n",
        "                           n_informative=5, n_clusters_per_class=4, \n",
        "                           random_state=42) #for reproducibility "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCr_IBuQO0SP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNB_ifrfO3V8"
      },
      "source": [
        "def dist(x,y):\n",
        "    return np.linalg.norm(x-y)\n",
        "    #return scipy.spatial.distance.correlation(x,y)\n",
        "\n",
        "def poprow(arr,i):\n",
        "    pop = arr[i]\n",
        "    new_array = np.vstack((arr[:i],arr[i+1:]))\n",
        "    return new_array,pop\n",
        "\n",
        "def cluster(X, p, k, dist_to_xr):\n",
        "    #c = [p]\n",
        "    #D = np.column_stack((X,[dist(v[:-1],p[:-1]) for v in X]))\n",
        "    #D = D[D[:,-1].argsort()]\n",
        "    #D = np.delete(D, -1, 1)\n",
        "    #c.extend(D[:k-1])\n",
        "    #D = D[k-1:]\n",
        "\n",
        "    #xc = np.array([p[:-1] for p in c], copy=False, ndmin=2)\n",
        "    #yc = np.array([p[-1] for p in c], copy=False)\n",
        "    #cl = (xc, yc)\n",
        "    #return D, cl\n",
        "\n",
        "    c = [p]\n",
        "    \n",
        "    if dist_to_xr == None:\n",
        "        distances = [dist(v[:-1],p[:-1]) for v in X]\n",
        "    else:\n",
        "        distances = dist_to_xr\n",
        "        \n",
        "    X = X[np.argpartition(distances, k-1)]\n",
        "    c.extend(X[:k-1])\n",
        "    X = X[k-1:]\n",
        "    \n",
        "    xc = np.array([p[:-1] for p in c], copy=False, ndmin=2)\n",
        "    yc = np.array([p[-1] for p in c], copy=False)\n",
        "    cl = (xc, yc)\n",
        "    \n",
        "    return X, cl\n",
        "    \n",
        "def mdav(X, y, k):\n",
        "    D = np.column_stack((X,y))\n",
        "    clusters = []\n",
        "    while len(D) >= 3*k:\n",
        "        # Centroid\n",
        "        xm = np.mean(D, axis=0)\n",
        "        # Furthest from centroid\n",
        "        xri = np.argmax([dist(v[:-1],xm[:-1]) for v in D])\n",
        "        D, xr = poprow(D, xri)\n",
        "        # Furthest from furthest from centroid\n",
        "        dist_to_xr = [dist(v[:-1],xr[:-1]) for v in D]\n",
        "        xsi = np.argmax(dist_to_xr)\n",
        "        dist_to_xr = dist_to_xr[:xsi]+dist_to_xr[xsi+1:]\n",
        "        D, xs = poprow(D, xsi) \n",
        "\n",
        "        #cluster of xr\n",
        "        D, c = cluster(D, xr, k, dist_to_xr)\n",
        "        clusters.append(c)\n",
        "        #cluster of xs\n",
        "        D, c = cluster(D, xs, k, None)\n",
        "        clusters.append(c)\n",
        "        \n",
        "    if len(D) >= 2*k and len(D) < 3*k:\n",
        "        # Centroid\n",
        "        xm = np.mean(D, axis=0)\n",
        "        # Furthest from centroid\n",
        "        xri = np.argmax([dist(v[:-1],xm[:-1]) for v in D])\n",
        "        D, xr = poprow(D, xri)\n",
        "        #cluster of xr\n",
        "        D, c = cluster(D, xr, k, None)\n",
        "        clusters.append(c)\n",
        "        \n",
        "        # rest of points\n",
        "        xc = np.array([p[:-1] for p in D[:]], copy=False, ndmin=2)\n",
        "        yc = np.array([p[-1] for p in D[:]], copy=False)\n",
        "        cl = (xc, yc)\n",
        "        clusters.append(cl)     \n",
        "    else:\n",
        "        # rest of points\n",
        "        xc = np.array([p[:-1] for p in D[:]], copy=False, ndmin=2)\n",
        "        yc = np.array([p[-1] for p in D[:]], copy=False)\n",
        "        cl = (xc, yc)\n",
        "        clusters.append(cl)\n",
        "    \n",
        "    centroids = np.array([np.mean(c[0],axis=0) for c in clusters], copy=False)\n",
        "    \n",
        "    return clusters, centroids\n",
        "\n",
        "from sklearn import tree\n",
        "def gen_explanations(clustering, max_depth=-1):\n",
        "    explanations = []\n",
        "    for cluster in clustering:\n",
        "        # Testing with max depth\n",
        "        if max_depth < 1:\n",
        "            exp = tree.DecisionTreeClassifier()\n",
        "        else:\n",
        "            exp = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
        "        exp.fit(cluster[0],cluster[1])\n",
        "        explanations.append(exp) \n",
        "    return explanations\n",
        "\n",
        "def pre_explanations(explanations, centroids, X):\n",
        "    predictions = []\n",
        "    for sample in X:\n",
        "        #select the closest classifier\n",
        "        exp = explanations[np.argmin([dist(sample,c) for c in centroids])]\n",
        "        exp_pred = exp.predict([sample])\n",
        "        predictions.append(int(exp_pred[0]))\n",
        "    return predictions\n",
        "\n",
        "def pre_explanations_ext(explanations, centroids, X, T, n):\n",
        "    predictions = []\n",
        "    ret_exp = []\n",
        "    ret_cen = []\n",
        "    for sample, truth in zip(X,T):\n",
        "        #select the 3 closest classifiers\n",
        "        mins = np.array([dist(sample,c) for c in centroids]).argsort()[:n]\n",
        "        for m in mins:\n",
        "            exp = explanations[m]\n",
        "            exp_pred = exp.predict([sample])\n",
        "            if(exp_pred[0] == truth):\n",
        "                break\n",
        "        predictions.append(exp_pred[0])\n",
        "        ret_exp.append(exp)\n",
        "        ret_cen.append(centroids[m])\n",
        "    return predictions, ret_exp, ret_cen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XGLvnfnPVDJ"
      },
      "source": [
        "import time\n",
        "exec_times = []\n",
        "\n",
        "# Generate clusters for different representativities\n",
        "#representativity = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "representativity = [0.1, 0.5]\n",
        "K = [int(len(X_train)*r) for r in representativity]\n",
        "clusterings = []\n",
        "centroids_of_clusterings = []\n",
        "for k in K:\n",
        "    start = time.time()\n",
        "    \n",
        "    clustering, centroids = mdav(X_train, y_train, k)\n",
        "    print(clustering)\n",
        "    clusterings.append(clustering)\n",
        "    centroids_of_clusterings.append(centroids)  \n",
        "    \n",
        "    end = time.time()\n",
        "    exec_times.append(end-start)\n",
        "    display(f'Time for k={k}: {end-start} seconds.')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}