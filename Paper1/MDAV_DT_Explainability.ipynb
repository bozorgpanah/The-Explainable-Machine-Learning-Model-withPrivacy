{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bozorgpanah/The-Explainable-Machine-Learning-Model-withPrivacy/blob/main/Paper1/MDAV_DT_Explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-InYE9O2znvV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import scipy\n",
        "%matplotlib inline\n",
        "\n",
        "X, y = make_classification(n_samples=1000000, n_features=10, n_redundant=3, n_repeated=2, \n",
        "                           n_informative=5, n_clusters_per_class=4, \n",
        "                           random_state=42) #for reproducibility "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duslnuvGznvZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OJY3UbTznvb"
      },
      "outputs": [],
      "source": [
        "def dist(x,y):\n",
        "    return np.linalg.norm(x-y)\n",
        "    #return scipy.spatial.distance.correlation(x,y)\n",
        "\n",
        "def poprow(arr,i):\n",
        "    pop = arr[i]\n",
        "    new_array = np.vstack((arr[:i],arr[i+1:]))\n",
        "    return new_array,pop\n",
        "\n",
        "def cluster(X, p, k, dist_to_xr):\n",
        "    #c = [p]\n",
        "    #D = np.column_stack((X,[dist(v[:-1],p[:-1]) for v in X]))\n",
        "    #D = D[D[:,-1].argsort()]\n",
        "    #D = np.delete(D, -1, 1)\n",
        "    #c.extend(D[:k-1])\n",
        "    #D = D[k-1:]\n",
        "\n",
        "    #xc = np.array([p[:-1] for p in c], copy=False, ndmin=2)\n",
        "    #yc = np.array([p[-1] for p in c], copy=False)\n",
        "    #cl = (xc, yc)\n",
        "    #return D, cl\n",
        "\n",
        "    c = [p]\n",
        "    \n",
        "    if dist_to_xr == None:\n",
        "        distances = [dist(v[:-1],p[:-1]) for v in X]\n",
        "    else:\n",
        "        distances = dist_to_xr\n",
        "        \n",
        "    X = X[np.argpartition(distances, k-1)]\n",
        "    c.extend(X[:k-1])\n",
        "    X = X[k-1:]\n",
        "    \n",
        "    xc = np.array([p[:-1] for p in c], copy=False, ndmin=2)\n",
        "    yc = np.array([p[-1] for p in c], copy=False)\n",
        "    cl = (xc, yc)\n",
        "    \n",
        "    return X, cl\n",
        "    \n",
        "def mdav(X, y, k):\n",
        "    D = np.column_stack((X,y))\n",
        "    clusters = []\n",
        "    while len(D) >= 3*k:\n",
        "        # Centroid\n",
        "        xm = np.mean(D, axis=0)\n",
        "        # Furthest from centroid\n",
        "        xri = np.argmax([dist(v[:-1],xm[:-1]) for v in D])\n",
        "        D, xr = poprow(D, xri)\n",
        "        # Furthest from furthest from centroid\n",
        "        dist_to_xr = [dist(v[:-1],xr[:-1]) for v in D]\n",
        "        xsi = np.argmax(dist_to_xr)\n",
        "        dist_to_xr = dist_to_xr[:xsi]+dist_to_xr[xsi+1:]\n",
        "        D, xs = poprow(D, xsi) \n",
        "\n",
        "        #cluster of xr\n",
        "        D, c = cluster(D, xr, k, dist_to_xr)\n",
        "        clusters.append(c)\n",
        "        #cluster of xs\n",
        "        D, c = cluster(D, xs, k, None)\n",
        "        clusters.append(c)\n",
        "        \n",
        "    if len(D) >= 2*k and len(D) < 3*k:\n",
        "        # Centroid\n",
        "        xm = np.mean(D, axis=0)\n",
        "        # Furthest from centroid\n",
        "        xri = np.argmax([dist(v[:-1],xm[:-1]) for v in D])\n",
        "        D, xr = poprow(D, xri)\n",
        "        #cluster of xr\n",
        "        D, c = cluster(D, xr, k, None)\n",
        "        clusters.append(c)\n",
        "        \n",
        "        # rest of points\n",
        "        xc = np.array([p[:-1] for p in D[:]], copy=False, ndmin=2)\n",
        "        yc = np.array([p[-1] for p in D[:]], copy=False)\n",
        "        cl = (xc, yc)\n",
        "        clusters.append(cl)     \n",
        "    else:\n",
        "        # rest of points\n",
        "        xc = np.array([p[:-1] for p in D[:]], copy=False, ndmin=2)\n",
        "        yc = np.array([p[-1] for p in D[:]], copy=False)\n",
        "        cl = (xc, yc)\n",
        "        clusters.append(cl)\n",
        "    \n",
        "    centroids = np.array([np.mean(c[0],axis=0) for c in clusters], copy=False)\n",
        "    \n",
        "    return clusters, centroids\n",
        "\n",
        "from sklearn import tree\n",
        "def gen_explanations(clustering, max_depth=-1):\n",
        "    explanations = []\n",
        "    for cluster in clustering:\n",
        "        # Testing with max depth\n",
        "        if max_depth < 1:\n",
        "            exp = tree.DecisionTreeClassifier()\n",
        "        else:\n",
        "            exp = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
        "        exp.fit(cluster[0],cluster[1])\n",
        "        explanations.append(exp) \n",
        "    return explanations\n",
        "\n",
        "def pre_explanations(explanations, centroids, X):\n",
        "    predictions = []\n",
        "    for sample in X:\n",
        "        #select the closest classifier\n",
        "        exp = explanations[np.argmin([dist(sample,c) for c in centroids])]\n",
        "        exp_pred = exp.predict([sample])\n",
        "        predictions.append(int(exp_pred[0]))\n",
        "    return predictions\n",
        "\n",
        "def pre_explanations_ext(explanations, centroids, X, T, n):\n",
        "    predictions = []\n",
        "    ret_exp = []\n",
        "    ret_cen = []\n",
        "    for sample, truth in zip(X,T):\n",
        "        #select the 3 closest classifiers\n",
        "        mins = np.array([dist(sample,c) for c in centroids]).argsort()[:n]\n",
        "        for m in mins:\n",
        "            exp = explanations[m]\n",
        "            exp_pred = exp.predict([sample])\n",
        "            if(exp_pred[0] == truth):\n",
        "                break\n",
        "        predictions.append(exp_pred[0])\n",
        "        ret_exp.append(exp)\n",
        "        ret_cen.append(centroids[m])\n",
        "    return predictions, ret_exp, ret_cen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2H7Pnd0rznve",
        "outputId": "22ecea8d-a126-4622-c0a4-c9a4d4256530"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=670: 3324.0733137130737 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=3350: 660.8805754184723 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=6700: 334.5123198032379 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=33500: 70.16843175888062 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=67000: 37.46081590652466 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=100500: 25.281006574630737 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=134000: 19.951709508895874 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=167500: 17.19151210784912 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Time for k=201000: 12.53227424621582 seconds.'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import time\n",
        "exec_times = []\n",
        "\n",
        "# Generate clusters for different representativities\n",
        "representativity = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "#representativity = [0.05]\n",
        "K = [int(len(X_train)*r) for r in representativity]\n",
        "clusterings = []\n",
        "centroids_of_clusterings = []\n",
        "for k in K:\n",
        "    start = time.time()\n",
        "\n",
        "    clustering, centroids = mdav(X_train, y_train, k)\n",
        "    clusterings.append(clustering)\n",
        "    centroids_of_clusterings.append(centroids)  \n",
        "    \n",
        "    end = time.time()\n",
        "    exec_times.append(end-start)\n",
        "    display(f'Time for k={k}: {end-start} seconds.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9xbw4seoznvh",
        "outputId": "3ef4e33a-6b37-4321-8f88-72144cde5af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3237.965715408325, 646.1582026481628, 321.6543471813202, 67.97480773925781, 36.26726269721985, 24.53523302078247, 19.540770292282104, 16.74518656730652, 12.1389479637146]\n"
          ]
        }
      ],
      "source": [
        "print(exec_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqnYZPSnznvj"
      },
      "outputs": [],
      "source": [
        "shallow=False\n",
        "clu_explanations = []\n",
        "for clustering in clusterings:\n",
        "    # Test with shallow trees depht=4\n",
        "    if shallow:\n",
        "        explanations = gen_explanations(clustering, 4)\n",
        "    else:\n",
        "        explanations = gen_explanations(clustering, -1)\n",
        "    clu_explanations.append(explanations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UF1llGDGznvl",
        "outputId": "fe6927d4-74b0-4269-85d0-04f8fab1a668",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.36259298\n",
            "Iteration 2, loss = 0.24581898\n",
            "Iteration 3, loss = 0.21294277\n",
            "Iteration 4, loss = 0.19200301\n",
            "Iteration 5, loss = 0.17568969\n",
            "Iteration 6, loss = 0.16444348\n",
            "Iteration 7, loss = 0.15690228\n",
            "Iteration 8, loss = 0.15077435\n",
            "Iteration 9, loss = 0.14606117\n",
            "Iteration 10, loss = 0.14222201\n",
            "Iteration 11, loss = 0.13912586\n",
            "Iteration 12, loss = 0.13637242\n",
            "Iteration 13, loss = 0.13414063\n",
            "Iteration 14, loss = 0.13199409\n",
            "Iteration 15, loss = 0.13024987\n",
            "Iteration 16, loss = 0.12856054\n",
            "Iteration 17, loss = 0.12700290\n",
            "Iteration 18, loss = 0.12602104\n",
            "Iteration 19, loss = 0.12463951\n",
            "Iteration 20, loss = 0.12366419\n",
            "Iteration 21, loss = 0.12262749\n",
            "Iteration 22, loss = 0.12173496\n",
            "Iteration 23, loss = 0.12085035\n",
            "Iteration 24, loss = 0.12035285\n",
            "Iteration 25, loss = 0.11947145\n",
            "Iteration 26, loss = 0.11879139\n",
            "Iteration 27, loss = 0.11831097\n",
            "Iteration 28, loss = 0.11753188\n",
            "Iteration 29, loss = 0.11700097\n",
            "Iteration 30, loss = 0.11643785\n",
            "Iteration 31, loss = 0.11635304\n",
            "Iteration 32, loss = 0.11577817\n",
            "Iteration 33, loss = 0.11525354\n",
            "Iteration 34, loss = 0.11469211\n",
            "Iteration 35, loss = 0.11459520\n",
            "Iteration 36, loss = 0.11412329\n",
            "Iteration 37, loss = 0.11366397\n",
            "Iteration 38, loss = 0.11331989\n",
            "Iteration 39, loss = 0.11315010\n",
            "Iteration 40, loss = 0.11281850\n",
            "Iteration 41, loss = 0.11245626\n",
            "Iteration 42, loss = 0.11222543\n",
            "Iteration 43, loss = 0.11191918\n",
            "Iteration 44, loss = 0.11160288\n",
            "Iteration 45, loss = 0.11132389\n",
            "Iteration 46, loss = 0.11107693\n",
            "Iteration 47, loss = 0.11085633\n",
            "Iteration 48, loss = 0.11066306\n",
            "Iteration 49, loss = 0.11053076\n",
            "Iteration 50, loss = 0.11020027\n",
            "Iteration 51, loss = 0.11006985\n",
            "Iteration 52, loss = 0.10988650\n",
            "Iteration 53, loss = 0.10965314\n",
            "Iteration 54, loss = 0.10953571\n",
            "Iteration 55, loss = 0.10932756\n",
            "Iteration 56, loss = 0.10908394\n",
            "Iteration 57, loss = 0.10919006\n",
            "Iteration 58, loss = 0.10890215\n",
            "Iteration 59, loss = 0.10884634\n",
            "Iteration 60, loss = 0.10854266\n",
            "Iteration 61, loss = 0.10838678\n",
            "Iteration 62, loss = 0.10814638\n",
            "Iteration 63, loss = 0.10821822\n",
            "Iteration 64, loss = 0.10804889\n",
            "Iteration 65, loss = 0.10793261\n",
            "Iteration 66, loss = 0.10781369\n",
            "Iteration 67, loss = 0.10771974\n",
            "Iteration 68, loss = 0.10729616\n",
            "Iteration 69, loss = 0.10726836\n",
            "Iteration 70, loss = 0.10723335\n",
            "Iteration 71, loss = 0.10716960\n",
            "Iteration 72, loss = 0.10719286\n",
            "Iteration 73, loss = 0.10693409\n",
            "Iteration 74, loss = 0.10686782\n",
            "Iteration 75, loss = 0.10668666\n",
            "Iteration 76, loss = 0.10650705\n",
            "Iteration 77, loss = 0.10653841\n",
            "Iteration 78, loss = 0.10638778\n",
            "Iteration 79, loss = 0.10631184\n",
            "Iteration 80, loss = 0.10610827\n",
            "Iteration 81, loss = 0.10596939\n",
            "Iteration 82, loss = 0.10612095\n",
            "Iteration 83, loss = 0.10593988\n",
            "Iteration 84, loss = 0.10585880\n",
            "Iteration 85, loss = 0.10584615\n",
            "Iteration 86, loss = 0.10564806\n",
            "Iteration 87, loss = 0.10556013\n",
            "Iteration 88, loss = 0.10557328\n",
            "Iteration 89, loss = 0.10542722\n",
            "Iteration 90, loss = 0.10550672\n",
            "Iteration 91, loss = 0.10523612\n",
            "Iteration 92, loss = 0.10518362\n",
            "Iteration 93, loss = 0.10511485\n",
            "Iteration 94, loss = 0.10505220\n",
            "Iteration 95, loss = 0.10487028\n",
            "Iteration 96, loss = 0.10477499\n",
            "Iteration 97, loss = 0.10491320\n",
            "Iteration 98, loss = 0.10486799\n",
            "Iteration 99, loss = 0.10463104\n",
            "Iteration 100, loss = 0.10491016\n",
            "Iteration 101, loss = 0.10461250\n",
            "Iteration 102, loss = 0.10459748\n",
            "Iteration 103, loss = 0.10447503\n",
            "Iteration 104, loss = 0.10444726\n",
            "Iteration 105, loss = 0.10442689\n",
            "Iteration 106, loss = 0.10427292\n",
            "Iteration 107, loss = 0.10414625\n",
            "Iteration 108, loss = 0.10424991\n",
            "Iteration 109, loss = 0.10413696\n",
            "Iteration 110, loss = 0.10411886\n",
            "Iteration 111, loss = 0.10408506\n",
            "Iteration 112, loss = 0.10401614\n",
            "Iteration 113, loss = 0.10396528\n",
            "Iteration 114, loss = 0.10396363\n",
            "Iteration 115, loss = 0.10391188\n",
            "Iteration 116, loss = 0.10388750\n",
            "Iteration 117, loss = 0.10377185\n",
            "Iteration 118, loss = 0.10364182\n",
            "Iteration 119, loss = 0.10369277\n",
            "Iteration 120, loss = 0.10353349\n",
            "Iteration 121, loss = 0.10348006\n",
            "Iteration 122, loss = 0.10363664\n",
            "Iteration 123, loss = 0.10343873\n",
            "Iteration 124, loss = 0.10322160\n",
            "Iteration 125, loss = 0.10339527\n",
            "Iteration 126, loss = 0.10334488\n",
            "Iteration 127, loss = 0.10324248\n",
            "Iteration 128, loss = 0.10329221\n",
            "Iteration 129, loss = 0.10314715\n",
            "Iteration 130, loss = 0.10311450\n",
            "Iteration 131, loss = 0.10322686\n",
            "Iteration 132, loss = 0.10306591\n",
            "Iteration 133, loss = 0.10297292\n",
            "Iteration 134, loss = 0.10305782\n",
            "Iteration 135, loss = 0.10291970\n",
            "Iteration 136, loss = 0.10290279\n",
            "Iteration 137, loss = 0.10279795\n",
            "Iteration 138, loss = 0.10285773\n",
            "Iteration 139, loss = 0.10282791\n",
            "Iteration 140, loss = 0.10285202\n",
            "Iteration 141, loss = 0.10281354\n",
            "Iteration 142, loss = 0.10266026\n",
            "Iteration 143, loss = 0.10263599\n",
            "Iteration 144, loss = 0.10252322\n",
            "Iteration 145, loss = 0.10257898\n",
            "Iteration 146, loss = 0.10252938\n",
            "Iteration 147, loss = 0.10246226\n",
            "Iteration 148, loss = 0.10251020\n",
            "Iteration 149, loss = 0.10253200\n",
            "Iteration 150, loss = 0.10245027\n",
            "Iteration 151, loss = 0.10246229\n",
            "Iteration 152, loss = 0.10232463\n",
            "Iteration 153, loss = 0.10222629\n",
            "Iteration 154, loss = 0.10246404\n",
            "Iteration 155, loss = 0.10234825\n",
            "Iteration 156, loss = 0.10220755\n",
            "Iteration 157, loss = 0.10206512\n",
            "Iteration 158, loss = 0.10209997\n",
            "Iteration 159, loss = 0.10206116\n",
            "Iteration 160, loss = 0.10207783\n",
            "Iteration 161, loss = 0.10213594\n",
            "Iteration 162, loss = 0.10207446\n",
            "Iteration 163, loss = 0.10188175\n",
            "Iteration 164, loss = 0.10197422\n",
            "Iteration 165, loss = 0.10191427\n",
            "Iteration 166, loss = 0.10198044\n",
            "Iteration 167, loss = 0.10198537\n",
            "Iteration 168, loss = 0.10189174\n",
            "Iteration 169, loss = 0.10174038\n",
            "Iteration 170, loss = 0.10171001\n",
            "Iteration 171, loss = 0.10174402\n",
            "Iteration 172, loss = 0.10173208\n",
            "Iteration 173, loss = 0.10185181\n",
            "Iteration 174, loss = 0.10172339\n",
            "Iteration 175, loss = 0.10167668\n",
            "Iteration 176, loss = 0.10157150\n",
            "Iteration 177, loss = 0.10163842\n",
            "Iteration 178, loss = 0.10155505\n",
            "Iteration 179, loss = 0.10153860\n",
            "Iteration 180, loss = 0.10151731\n",
            "Iteration 181, loss = 0.10158315\n",
            "Iteration 182, loss = 0.10149351\n",
            "Iteration 183, loss = 0.10141938\n",
            "Iteration 184, loss = 0.10135028\n",
            "Iteration 185, loss = 0.10138507\n",
            "Iteration 186, loss = 0.10136023\n",
            "Iteration 187, loss = 0.10132208\n",
            "Iteration 188, loss = 0.10140618\n",
            "Iteration 189, loss = 0.10128054\n",
            "Iteration 190, loss = 0.10129985\n",
            "Iteration 191, loss = 0.10130181\n",
            "Iteration 192, loss = 0.10107698\n",
            "Iteration 193, loss = 0.10115174\n",
            "Iteration 194, loss = 0.10124183\n",
            "Iteration 195, loss = 0.10113185\n",
            "Iteration 196, loss = 0.10119546\n",
            "Iteration 197, loss = 0.10101427\n",
            "Iteration 198, loss = 0.10100095\n",
            "Iteration 199, loss = 0.10097494\n",
            "Iteration 200, loss = 0.10109091\n",
            "Iteration 201, loss = 0.10100063\n",
            "Iteration 202, loss = 0.10095519\n",
            "Iteration 203, loss = 0.10087000\n",
            "Iteration 204, loss = 0.10092507\n",
            "Iteration 205, loss = 0.10095814\n",
            "Iteration 206, loss = 0.10088105\n",
            "Iteration 207, loss = 0.10089603\n",
            "Iteration 208, loss = 0.10085980\n",
            "Iteration 209, loss = 0.10077195\n",
            "Iteration 210, loss = 0.10079509\n",
            "Iteration 211, loss = 0.10090962\n",
            "Iteration 212, loss = 0.10065830\n",
            "Iteration 213, loss = 0.10068099\n",
            "Iteration 214, loss = 0.10067975\n",
            "Iteration 215, loss = 0.10058758\n",
            "Iteration 216, loss = 0.10074769\n",
            "Iteration 217, loss = 0.10079139\n",
            "Iteration 218, loss = 0.10067453\n",
            "Iteration 219, loss = 0.10062749\n",
            "Iteration 220, loss = 0.10057203\n",
            "Iteration 221, loss = 0.10050347\n",
            "Iteration 222, loss = 0.10055300\n",
            "Iteration 223, loss = 0.10046498\n",
            "Iteration 224, loss = 0.10054989\n",
            "Iteration 225, loss = 0.10054008\n",
            "Iteration 226, loss = 0.10043681\n",
            "Iteration 227, loss = 0.10039089\n",
            "Iteration 228, loss = 0.10040507\n",
            "Iteration 229, loss = 0.10037267\n",
            "Iteration 230, loss = 0.10042847\n",
            "Iteration 231, loss = 0.10022837\n",
            "Iteration 232, loss = 0.10027186\n",
            "Iteration 233, loss = 0.10033820\n",
            "Iteration 234, loss = 0.10029703\n",
            "Iteration 235, loss = 0.10011811\n",
            "Iteration 236, loss = 0.10020856\n",
            "Iteration 237, loss = 0.10043942\n",
            "Iteration 238, loss = 0.10013326\n",
            "Iteration 239, loss = 0.10016885\n",
            "Iteration 240, loss = 0.10027200\n",
            "Iteration 241, loss = 0.10018234\n",
            "Iteration 242, loss = 0.10014810\n",
            "Iteration 243, loss = 0.10010592\n",
            "Iteration 244, loss = 0.10022582\n",
            "Iteration 245, loss = 0.10012212\n",
            "Iteration 246, loss = 0.09999794\n",
            "Iteration 247, loss = 0.10006274\n",
            "Iteration 248, loss = 0.10000846\n",
            "Iteration 249, loss = 0.10002498\n",
            "Iteration 250, loss = 0.09999979\n",
            "Iteration 251, loss = 0.09999899\n",
            "Iteration 252, loss = 0.09995366\n",
            "Iteration 253, loss = 0.09995198\n",
            "Iteration 254, loss = 0.09997090\n",
            "Iteration 255, loss = 0.09999350\n",
            "Iteration 256, loss = 0.10000196\n",
            "Iteration 257, loss = 0.09983690\n",
            "Iteration 258, loss = 0.09989858\n",
            "Iteration 259, loss = 0.09987614\n",
            "Iteration 260, loss = 0.09979602\n",
            "Iteration 261, loss = 0.09987740\n",
            "Iteration 262, loss = 0.09986170\n",
            "Iteration 263, loss = 0.09993682\n",
            "Iteration 264, loss = 0.09977865\n",
            "Iteration 265, loss = 0.09975587\n",
            "Iteration 266, loss = 0.09969543\n",
            "Iteration 267, loss = 0.09971659\n",
            "Iteration 268, loss = 0.09962320\n",
            "Iteration 269, loss = 0.09965901\n",
            "Iteration 270, loss = 0.09971654\n",
            "Iteration 271, loss = 0.09974325\n",
            "Iteration 272, loss = 0.09971123\n",
            "Iteration 273, loss = 0.09963630\n",
            "Iteration 274, loss = 0.09960098\n",
            "Iteration 275, loss = 0.09971217\n",
            "Iteration 276, loss = 0.09959604\n",
            "Iteration 277, loss = 0.09959626\n",
            "Iteration 278, loss = 0.09962300\n",
            "Iteration 279, loss = 0.09958740\n",
            "Iteration 280, loss = 0.09963160\n",
            "Iteration 281, loss = 0.09954049\n",
            "Iteration 282, loss = 0.09962882\n",
            "Iteration 283, loss = 0.09950885\n",
            "Iteration 284, loss = 0.09954923\n",
            "Iteration 285, loss = 0.09945867\n",
            "Iteration 286, loss = 0.09943248\n",
            "Iteration 287, loss = 0.09942717\n",
            "Iteration 288, loss = 0.09958820\n",
            "Iteration 289, loss = 0.09944546\n",
            "Iteration 290, loss = 0.09943069\n",
            "Iteration 291, loss = 0.09936197\n",
            "Iteration 292, loss = 0.09935238\n",
            "Iteration 293, loss = 0.09943162\n",
            "Iteration 294, loss = 0.09930318\n",
            "Iteration 295, loss = 0.09926506\n",
            "Iteration 296, loss = 0.09930416\n",
            "Iteration 297, loss = 0.09926479\n",
            "Iteration 298, loss = 0.09935835\n",
            "Iteration 299, loss = 0.09931055\n",
            "Iteration 300, loss = 0.09934991\n",
            "Iteration 301, loss = 0.09926424\n",
            "Iteration 302, loss = 0.09928555\n",
            "Iteration 303, loss = 0.09915013\n",
            "Iteration 304, loss = 0.09919810\n",
            "Iteration 305, loss = 0.09918177\n",
            "Iteration 306, loss = 0.09925362\n",
            "Iteration 307, loss = 0.09926148\n",
            "Iteration 308, loss = 0.09917194\n",
            "Iteration 309, loss = 0.09922788\n",
            "Iteration 310, loss = 0.09912019\n",
            "Iteration 311, loss = 0.09909310\n",
            "Iteration 312, loss = 0.09913475\n",
            "Iteration 313, loss = 0.09899679\n",
            "Iteration 314, loss = 0.09908957\n",
            "Iteration 315, loss = 0.09920054\n",
            "Iteration 316, loss = 0.09906700\n",
            "Iteration 317, loss = 0.09903439\n",
            "Iteration 318, loss = 0.09907334\n",
            "Iteration 319, loss = 0.09903400\n",
            "Iteration 320, loss = 0.09888489\n",
            "Iteration 321, loss = 0.09900116\n",
            "Iteration 322, loss = 0.09902213\n",
            "Iteration 323, loss = 0.09886626\n",
            "Iteration 324, loss = 0.09907230\n",
            "Iteration 325, loss = 0.09897776\n",
            "Iteration 326, loss = 0.09892850\n",
            "Iteration 327, loss = 0.09889850\n",
            "Iteration 328, loss = 0.09888255\n",
            "Iteration 329, loss = 0.09893120\n",
            "Iteration 330, loss = 0.09887706\n",
            "Iteration 331, loss = 0.09889139\n",
            "Iteration 332, loss = 0.09892548\n",
            "Iteration 333, loss = 0.09887209\n",
            "Iteration 334, loss = 0.09880997\n",
            "Iteration 335, loss = 0.09883251\n",
            "Iteration 336, loss = 0.09878246\n",
            "Iteration 337, loss = 0.09876240\n",
            "Iteration 338, loss = 0.09887916\n",
            "Iteration 339, loss = 0.09883580\n",
            "Iteration 340, loss = 0.09885433\n",
            "Iteration 341, loss = 0.09876733\n",
            "Iteration 342, loss = 0.09872378\n",
            "Iteration 343, loss = 0.09878707\n",
            "Iteration 344, loss = 0.09874620\n",
            "Iteration 345, loss = 0.09870236\n",
            "Iteration 346, loss = 0.09875572\n",
            "Iteration 347, loss = 0.09872843\n",
            "Iteration 348, loss = 0.09871100\n",
            "Iteration 349, loss = 0.09874790\n",
            "Iteration 350, loss = 0.09862461\n",
            "Iteration 351, loss = 0.09858992\n",
            "Iteration 352, loss = 0.09867693\n",
            "Iteration 353, loss = 0.09867354\n",
            "Iteration 354, loss = 0.09871385\n",
            "Iteration 355, loss = 0.09866236\n",
            "Iteration 356, loss = 0.09859484\n",
            "Iteration 357, loss = 0.09856944\n",
            "Iteration 358, loss = 0.09856844\n",
            "Iteration 359, loss = 0.09856072\n",
            "Iteration 360, loss = 0.09858650\n",
            "Iteration 361, loss = 0.09849957\n",
            "Iteration 362, loss = 0.09851503\n",
            "Iteration 363, loss = 0.09848998\n",
            "Iteration 364, loss = 0.09854572\n",
            "Iteration 365, loss = 0.09847612\n",
            "Iteration 366, loss = 0.09845562\n",
            "Iteration 367, loss = 0.09848891\n",
            "Iteration 368, loss = 0.09849799\n",
            "Iteration 369, loss = 0.09855424\n",
            "Iteration 370, loss = 0.09842961\n",
            "Iteration 371, loss = 0.09833555\n",
            "Iteration 372, loss = 0.09842777\n",
            "Iteration 373, loss = 0.09843063\n",
            "Iteration 374, loss = 0.09840057\n",
            "Iteration 375, loss = 0.09841022\n",
            "Iteration 376, loss = 0.09838219\n",
            "Iteration 377, loss = 0.09845303\n",
            "Iteration 378, loss = 0.09832421\n",
            "Iteration 379, loss = 0.09841766\n",
            "Iteration 380, loss = 0.09832418\n",
            "Iteration 381, loss = 0.09844293\n",
            "Iteration 382, loss = 0.09838797\n",
            "Iteration 383, loss = 0.09832672\n",
            "Iteration 384, loss = 0.09817525\n",
            "Iteration 385, loss = 0.09825151\n",
            "Iteration 386, loss = 0.09826668\n",
            "Iteration 387, loss = 0.09826290\n",
            "Iteration 388, loss = 0.09825982\n",
            "Iteration 389, loss = 0.09829675\n",
            "Iteration 390, loss = 0.09834481\n",
            "Iteration 391, loss = 0.09824257\n",
            "Iteration 392, loss = 0.09824268\n",
            "Iteration 393, loss = 0.09821235\n",
            "Iteration 394, loss = 0.09835299\n",
            "Iteration 395, loss = 0.09813144\n",
            "Iteration 396, loss = 0.09825553\n",
            "Iteration 397, loss = 0.09816468\n",
            "Iteration 398, loss = 0.09817250\n",
            "Iteration 399, loss = 0.09816189\n",
            "Iteration 400, loss = 0.09808398\n",
            "Iteration 401, loss = 0.09815603\n",
            "Iteration 402, loss = 0.09810005\n",
            "Iteration 403, loss = 0.09801372\n",
            "Iteration 404, loss = 0.09816839\n",
            "Iteration 405, loss = 0.09823356\n",
            "Iteration 406, loss = 0.09816779\n",
            "Iteration 407, loss = 0.09816003\n",
            "Iteration 408, loss = 0.09816524\n",
            "Iteration 409, loss = 0.09807383\n",
            "Iteration 410, loss = 0.09803007\n",
            "Iteration 411, loss = 0.09810423\n",
            "Iteration 412, loss = 0.09804714\n",
            "Iteration 413, loss = 0.09800076\n",
            "Iteration 414, loss = 0.09799020\n",
            "Iteration 415, loss = 0.09799661\n",
            "Iteration 416, loss = 0.09801916\n",
            "Iteration 417, loss = 0.09795562\n",
            "Iteration 418, loss = 0.09800317\n",
            "Iteration 419, loss = 0.09792241\n",
            "Iteration 420, loss = 0.09800340\n",
            "Iteration 421, loss = 0.09790850\n",
            "Iteration 422, loss = 0.09792908\n",
            "Iteration 423, loss = 0.09801134\n",
            "Iteration 424, loss = 0.09788190\n",
            "Iteration 425, loss = 0.09789753\n",
            "Iteration 426, loss = 0.09792055\n",
            "Iteration 427, loss = 0.09786085\n",
            "Iteration 428, loss = 0.09790888\n",
            "Iteration 429, loss = 0.09788421\n",
            "Iteration 430, loss = 0.09792666\n",
            "Iteration 431, loss = 0.09789951\n",
            "Iteration 432, loss = 0.09787856\n",
            "Iteration 433, loss = 0.09785115\n",
            "Iteration 434, loss = 0.09795210\n",
            "Iteration 435, loss = 0.09777122\n",
            "Iteration 436, loss = 0.09785339\n",
            "Iteration 437, loss = 0.09782776\n",
            "Iteration 438, loss = 0.09782666\n",
            "Iteration 439, loss = 0.09788381\n",
            "Iteration 440, loss = 0.09780561\n",
            "Iteration 441, loss = 0.09786839\n",
            "Iteration 442, loss = 0.09783273\n",
            "Iteration 443, loss = 0.09788175\n",
            "Iteration 444, loss = 0.09780983\n",
            "Iteration 445, loss = 0.09765688\n",
            "Iteration 446, loss = 0.09775716\n",
            "Iteration 447, loss = 0.09768727\n",
            "Iteration 448, loss = 0.09769411\n",
            "Iteration 449, loss = 0.09775937\n",
            "Iteration 450, loss = 0.09770527\n",
            "Iteration 451, loss = 0.09773533\n",
            "Iteration 452, loss = 0.09783320\n",
            "Iteration 453, loss = 0.09778762\n",
            "Iteration 454, loss = 0.09774860\n",
            "Iteration 455, loss = 0.09772172\n",
            "Iteration 456, loss = 0.09769018\n",
            "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
              "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train blackbox model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "blackbox = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\n",
        "blackbox.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bLO1Vw6znvn",
        "outputId": "c32160ee-5863-4ab7-aa6c-bcafbbf047fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9188575757575758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "71617"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Train big tree\n",
        "surrogate = tree.DecisionTreeClassifier()\n",
        "surrogate.fit(X_train,y_train)\n",
        "display(surrogate.score(X_test, y_test))\n",
        "display(surrogate.tree_.node_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6R00DCTznvp",
        "outputId": "63a84954-3274-4564-c967-f5b8811f6908"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\alberto blanco\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(surrogate, out_file=dot_data,  \n",
        "                filled=True, rounded=True, #feature_names=labels, class_names=['below 50K', 'over 50K'],\n",
        "                special_characters=True)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "display(Image(graph.create_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gYsVjUHznvs"
      },
      "outputs": [],
      "source": [
        "truth = y_test\n",
        "blackbox_predictions = blackbox.predict(X_test)\n",
        "surrogate_predictions = surrogate.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9hd8u-Qznvt"
      },
      "outputs": [],
      "source": [
        "explanation_predictions = []\n",
        "explanation_ext_predictions = []\n",
        "for i in range(len(clu_explanations)):\n",
        "    p = pre_explanations(clu_explanations[i], centroids_of_clusterings[i], X_test)\n",
        "    q,_,_ = pre_explanations_ext(clu_explanations[i], centroids_of_clusterings[i], X_test, blackbox_predictions, 3)\n",
        "    explanation_predictions.append(p)\n",
        "    explanation_ext_predictions.append(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "049Xeugvznvv",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# acc = TP+TN/all\n",
        "acc_blackbox = np.mean([t==p for t,p in zip(truth, blackbox_predictions)])\n",
        "acc_surrogate = np.mean([t==p for t,p in zip(truth, surrogate_predictions)])\n",
        "acc_explanations = []\n",
        "for i in range(len(explanation_predictions)):\n",
        "    acc_explanations.append(np.mean([t==p for t,p in zip(truth, explanation_predictions[i])]))\n",
        "    \n",
        "acc_explanations_ext = []\n",
        "for i in range(len(explanation_ext_predictions)):\n",
        "    acc_explanations_ext.append(np.mean([t==p for t,p in zip(truth, explanation_ext_predictions[i])]))\n",
        "\n",
        "acc_cross = []\n",
        "for i in range(len(explanation_predictions)):\n",
        "    acc_cross.append(np.mean([t==p for t,p in zip(blackbox_predictions, explanation_predictions[i])]))\n",
        "\n",
        "acc_cross_ext = []\n",
        "for i in range(len(explanation_predictions)):\n",
        "    acc_cross_ext.append(np.mean([t==p for t,p in zip(blackbox_predictions, explanation_ext_predictions[i])]))\n",
        "\n",
        "display(acc_blackbox)\n",
        "display(acc_surrogate)\n",
        "display(acc_explanations)\n",
        "display(acc_explanations_ext)\n",
        "display(acc_cross)\n",
        "display(acc_cross_ext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GSvfUrrznvy"
      },
      "outputs": [],
      "source": [
        "min_nodes = []\n",
        "max_nodes = []\n",
        "mean_nodes = []\n",
        "median_nodes = []\n",
        "n_counts = []\n",
        "for explanations in clu_explanations:\n",
        "    node_counts = [exp.tree_.node_count for exp in explanations]\n",
        "    n_counts.append(node_counts)\n",
        "    min_nodes.append(np.min(node_counts))\n",
        "    max_nodes.append(np.max(node_counts))\n",
        "    mean_nodes.append(np.mean(node_counts))\n",
        "    median_nodes.append(np.median(node_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWBt_ZGKznv0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "\n",
        "representativity = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "representativity = list([r*100 for r in representativity])\n",
        "representativity = representativity\n",
        "\n",
        "xnew = np.linspace(min(representativity),max(representativity),300)\n",
        "spl = make_interp_spline(representativity, acc_explanations, k=3) #BSpline object\n",
        "ynew = spl(xnew)\n",
        "\n",
        "spl = make_interp_spline(representativity, acc_explanations_ext, k=3) #BSpline object\n",
        "ynew4 = spl(xnew)\n",
        "\n",
        "acc_blackbox_const = [acc_blackbox]*len(representativity)\n",
        "spl = make_interp_spline(representativity, acc_blackbox_const, k=3) #BSpline object\n",
        "ynew2 = spl(xnew)\n",
        "\n",
        "big_tree = [acc_surrogate]*len(representativity)\n",
        "spl = make_interp_spline(representativity, big_tree, k=3) #BSpline object\n",
        "ynew6 = spl(xnew)\n",
        "\n",
        "spl = make_interp_spline(representativity, acc_cross, k=3) #BSpline object\n",
        "ynew3 = spl(xnew)\n",
        "\n",
        "spl = make_interp_spline(representativity, acc_cross_ext, k=3) #BSpline object\n",
        "ynew5 = spl(xnew)\n",
        "\n",
        "spl = make_interp_spline(representativity, exec_times, k=3)\n",
        "ynew7 = spl(xnew)\n",
        "\n",
        "plt.figure(figsize=(9,7))\n",
        "#plt.plot(xnew,ynew2,linestyle='-',linewidth=3,color='k',label='ANN')\n",
        "#plt.plot(xnew,ynew6,linestyle='-.',linewidth=3,color='k',label='Global DT')\n",
        "#plt.plot(xnew,ynew,linestyle=':',linewidth=3,color='k',label='Unguided DT')\n",
        "#plt.plot(xnew,ynew4,linestyle='--',linewidth=3,color='k',label='Guided DT')\n",
        "\n",
        "plt.plot(representativity,acc_blackbox_const,linestyle='-',linewidth=3,color='k',label='ANN')\n",
        "plt.plot(representativity,big_tree,linestyle='-.',linewidth=3,color='k',label='Global DT')\n",
        "plt.plot(representativity,acc_explanations,linestyle=':',linewidth=3,color='k',label='Unguided DT')\n",
        "plt.plot(representativity,acc_explanations_ext,linestyle='--',linewidth=3,color='k',label='Guided DT')\n",
        "\n",
        "plt.legend(handlelength=4)\n",
        "plt.ylim(0.8,1)\n",
        "plt.xlabel('% of records per cluster')\n",
        "plt.ylabel('classification accuracy')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwhlU70dznv6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,7))\n",
        "#plt.plot(xnew,ynew3,linestyle='-',linewidth=3,color='k',label='Unguided DT w.r.t. ANN predictions')\n",
        "#plt.plot(xnew,ynew5,linestyle=':',linewidth=3,color='k',label='Guided DT w.r.t. ANN predictions')\n",
        "\n",
        "plt.plot(representativity,acc_cross,linestyle=':',linewidth=3,color='k',label='Unguided DT w.r.t. ANN predictions')\n",
        "plt.plot(representativity,acc_cross_ext,linestyle='--',linewidth=3,color='k',label='Guided DT w.r.t. ANN predictions')\n",
        "\n",
        "plt.legend()\n",
        "plt.ylim(0.8,1)\n",
        "plt.xlabel('% of records per cluster')\n",
        "plt.ylabel('classification accuracy')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz48Vpn1znv8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,7))\n",
        "plt.boxplot(n_counts[:9])\n",
        "locs, _ = plt.xticks()\n",
        "plt.xticks(locs, representativity)\n",
        "if not shallow:\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel('number of nodes (log scale)')\n",
        "else:\n",
        "    plt.ylabel('number of nodes')\n",
        "\n",
        "plt.xlabel('% of records per cluster')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArDFeM1aznv9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,7))\n",
        "plt.plot(representativity,exec_times,linestyle='-',linewidth=3,color='k')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('% of records per cluster')\n",
        "plt.ylabel('execution time (s)')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcvRFXbpznv_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "MDAV-DT-Explainability.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}