{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDAV+SHAP+2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwtrifX5sUrg1T4E/GSqeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bozorgpanah/The-Explainable-Machine-Learning-Model-withPrivacy/blob/main/Paper1/MDAV%2BSHAP%2B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uBp-QvdPgex"
      },
      "source": [
        "#Split matrices into random train and test subsets_Devided data set in two parts include train and test parts 70%-30% respectively\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import scipy\n",
        "%matplotlib inline\n",
        "from google.colab import files\n",
        "import seaborn as sn\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_redundant=3, n_repeated=2, \n",
        "                           n_informative=5, n_clusters_per_class=4, \n",
        "                           random_state=42) #for reproducibility \n",
        "\n",
        "plt.figure(figsize=(50,50))\n",
        "plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n",
        "\n",
        "plt.subplot(321)\n",
        "plt.title(\"Five informative feature, Four clusters per class\", fontsize='small')\n",
        "\n",
        "#print (X ,y)\n",
        "plt.scatter(X[:, 0], X[:, 1], marker='s', c=y, s=25, edgecolor='c')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Making file to save data\n",
        "fX = open(\"Dataset_X.txt\",\"wt\")\n",
        "fX.write(\"\\n\"+str(X)+\"\\n\")\n",
        "fy = open(\"Dataset_y.txt\",\"wt\")\n",
        "fy.write(\"\\n\"+str(y)+\"\\n\")\n",
        "fX.close()\n",
        "fy.close()\n",
        "files.download(\"Dataset_X.txt\")\n",
        "files.download(\"Dataset_y.txt\")\n",
        "\n",
        "z = X.mean()\n",
        "print (z)\n",
        "covMatrix = np.cov(X)\n",
        "'''\n",
        "sn.heatmap(covMatrix, annot=True, fmt='g')\n",
        "plt.show()\n",
        "'''\n",
        "print(covMatrix)\n",
        "#Making file to save data\n",
        "CovRealData = open(\"covMatrix_RealData.txt\",\"wt\")\n",
        "CovRealData.write(\"\\n\"+str(covMatrix)+\"\\n\")\n",
        "CovRealData.close()\n",
        "files.download(\"covMatrix_RealData.txt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgrRdqi0Qwfh"
      },
      "source": [
        "#Split matrices into random train and test subsets_Devided data set in two parts include train and test parts 70%-30% respectively\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWFpBhhOQzAO"
      },
      "source": [
        "#training data\n",
        "print (X_train ,y_train)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], marker='s', c=y_train, s=25, edgecolor='c')\n",
        "plt.show()\n",
        "\n",
        "#test data\n",
        "print (X_test ,y_test)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], marker='s', c=y_test, s=25, edgecolor='c')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i3geH3YQ1fc"
      },
      "source": [
        "#MDAV a microagregation method\n",
        "def dist(x,y):\n",
        "    return np.linalg.norm(x-y)\n",
        "    \n",
        "#Stack arrays in sequence vertically (row wise)\n",
        "def poprow(arr,i):\n",
        "    pop = arr[i]\n",
        "    new_array = np.vstack((arr[:i],arr[i+1:]))\n",
        "    return new_array,pop\n",
        "\n",
        "def cluster(X, p, k, dist_to_xr):\n",
        "    c = [p]\n",
        "    \n",
        "    if dist_to_xr == None:\n",
        "        distances = [dist(v[:-1],p[:-1]) for v in X]\n",
        "    else:\n",
        "        distances = dist_to_xr\n",
        "        \n",
        "    X = X[np.argpartition(distances, k-1)]\n",
        "    c.extend(X[:k-1])\n",
        "    X = X[k-1:]\n",
        "    \n",
        "    xc = np.array([p[:-1] for p in c], copy=False, ndmin=2)\n",
        "    yc = np.array([p[-1] for p in c], copy=False)\n",
        "    cl = (xc, yc)\n",
        "    \n",
        "    return X, cl\n",
        "\n",
        "def mdav(X, y, k):\n",
        "    D = np.column_stack((X,y)) #D is a matrix of variables\n",
        "    clusters = []\n",
        "    centroids = []\n",
        "    while len(D) >= 3*k:\n",
        "        # Centroid\n",
        "        xm = np.mean(D, axis=0) #xm is an array includes mean in each variables (each columns)\n",
        "        # Furthest from centroid\n",
        "        xri = np.argmax([dist(v[:-1],xm[:-1]) for v in D])#Find furthest from the centroid in each vector and it's called xri\n",
        "        D, xr = poprow(D, xri)\n",
        "        # Furthest from furthest from centroid\n",
        "        dist_to_xr = [dist(v[:-1],xr[:-1]) for v in D]\n",
        "        xsi = np.argmax(dist_to_xr)\n",
        "        dist_to_xr = dist_to_xr[:xsi]+dist_to_xr[xsi+1:]\n",
        "        D, xs = poprow(D, xsi) \n",
        "\n",
        "        #cluster of xr\n",
        "        D, c = cluster(D, xr, k, dist_to_xr)\n",
        "        clusters.append(c)\n",
        "        #cluster of xs\n",
        "        D, c = cluster(D, xs, k, None)\n",
        "        clusters.append(c)\n",
        "        \n",
        "    if len(D) >= 2*k and len(D) < 3*k:\n",
        "        # Centroid\n",
        "        xm = np.mean(D, axis=0)\n",
        "        # Furthest from centroid\n",
        "        xri = np.argmax([dist(v[:-1],xm[:-1]) for v in D])\n",
        "        D, xr = poprow(D, xri)\n",
        "        #cluster of xr\n",
        "        D, c = cluster(D, xr, k, None)\n",
        "        clusters.append(c)\n",
        "        \n",
        "        # rest of points\n",
        "        xc = np.array([p[:-1] for p in D[:]], copy=False, ndmin=2)\n",
        "        yc = np.array([p[-1] for p in D[:]], copy=False)\n",
        "        cl = (xc, yc)\n",
        "        clusters.append(cl)     \n",
        "    else:\n",
        "        # rest of points\n",
        "        xc = np.array([p[:-1] for p in D[:]], copy=False, ndmin=2)\n",
        "        yc = np.array([p[-1] for p in D[:]], copy=False)\n",
        "        cl = (xc, yc)\n",
        "        clusters.append(cl)\n",
        "    \n",
        "    centroids = np.array([np.mean(c[0],axis=0) for c in clusters], copy=False)\n",
        "    \n",
        "    return clusters, centroids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXo3su1FQ5py"
      },
      "source": [
        "#11022020 We use smaller K amount\n",
        "import time\n",
        "import statistics \n",
        "exec_times = []\n",
        "\n",
        "# Generate clusters for different representativities\n",
        "#representativity = [0.001,0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, \n",
        "#                    0.009, 0.01, 0.02, 0.03]\n",
        "representativity = [0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09,0.1]\n",
        "K = [int(len(X)*r) for r in representativity]#K = [int(len(X_train)*r) for r in representativity]\n",
        "clusterings = []\n",
        "centroids = []\n",
        "maskedData = []\n",
        "print(f'k = {K}')\n",
        "\n",
        "for k in K:\n",
        "    start = time.time()\n",
        "    #print(k, K)\n",
        "    clustering, centroid = mdav(X, y, k)#clustering, centroid = mdav(X_train, y_train, k)\n",
        "    clusterings.append(clustering)\n",
        "    centroids.append(centroid) \n",
        "    i = 0\n",
        "    while i<k:\n",
        "     maskedData.append(centroid) \n",
        "     i += 1  \n",
        "\n",
        "    end = time.time()\n",
        "    exec_times.append(end-start)\n",
        "    #arr_ctr_num.append(len(centroids))\n",
        "    \n",
        "#print(f'Time for calculating = {exec_times}')\n",
        "\n",
        "#Saveing centroids in a file\n",
        "f_centroids = open(\"Centroids.txt\",\"wt\")\n",
        "f_centroids.write(\"\\n\"+str(centroids)+\"\\n\")\n",
        "f_clusters = open(\"Clusers.txt\",\"wt\")\n",
        "f_clusters.write(\"\\n\"+str(clusterings)+\"\\n\")\n",
        "f_maskedData = open(\"MaskedData.txt\",\"wt\")\n",
        "f_maskedData.write(\"\\n\"+str(maskedData)+\"\\n\")\n",
        "f_maskedData.close()\n",
        "f_centroids.close()\n",
        "f_clusters.close()\n",
        "files.download(\"MaskedData.txt\")\n",
        "files.download(\"Centroids.txt\")\n",
        "files.download(\"Clusers.txt\")\n",
        "#dr = np.array([4,2,1,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0,0])\n",
        "#k = np.array([8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48 , 52])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}